#Author : Abhishek Bajpai (abhishek.bajpai.ca@gmail.com)
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input

# Define the directories
#train_data_dir = 'data/training/people' #data\training\people
# Since I am using volumn mapped to container..hence app/ 
train_data_dir = '../data/training/animals' #data\training\people
print(f'~~~~~~ Current Working Directory is ~~~~~~~~~ {os.getcwd()}')
# Preprocessing parameters
target_size = (150, 150)
batch_size = 32

# Define preprocessing and augmentation parameters
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # You can add more preprocessing/augmentation options here

# Generate batches of augmented data from the directory
image_height, image_width = 150, 150  # Specify the target size for resizing
batch_size = 32

# Generate batches of augmented data from the directory
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(image_height, image_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

# Define the input shape
input_shape = (150, 150, 3)

# Define the model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),  # Flatten the output of the convolutional layers
    Dense(512, activation='relu'),
    Dense(len(train_generator.class_indices), activation='softmax')
])

# Compile the model
# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])


# Train the model
epochs = 30 # epochs is set to 10, meaning that the entire dataset will be passed through the network 10 times during training. During each epoch, the model is trained using the training data generated by train_generator.  adjust the value of epochs based on the performance of your model. Increasing the number of epochs may improve the model's performance, but be cautious of overfitting. If the model's performance on the validation data starts to decrease after a certain number of epochs, it may be a sign of overfitting, and you may need to stop training or adjust regularization techniques
history = model.fit(train_generator, epochs=epochs)

# Save the trained model
#model.save("AI-Projects\image-classification\animal_classifier_model.h5")
# Save the trained model using the native Keras format
model.save("trained_models/abeer_amiya_animals_classifier_model-v1.keras")
print("Model training completed and saved.")
